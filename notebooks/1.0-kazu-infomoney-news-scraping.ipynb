{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infomoney News Scraping\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the necessary imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from multiprocessing.managers import ListProxy\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, ResultSet, Tag\n",
    "from numpy import Series, nan\n",
    "from pandas import DataFrame\n",
    "from requests import Response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a `numpy` series contaning all the links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: DataFrame = pd.read_csv(\"infomoney-links.csv\")\n",
    "links: Any = df[\"link\"].values.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a multiprocess workflow to make requests and parse then using `BeautifulSoup` for the information that we're interested in the news.\n",
    "\n",
    "> An interest point of discution and improvement would be the usage of `lxml` instead of `html.parser`, since it should be faster.\n",
    "\n",
    "After each process finishes running we have a list of dicts containing all the information regarding that news.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(index, input_list, output_list):\n",
    "    url: Any = input_list[index]\n",
    "    response: Response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    paragraph: ResultSet[Tag] = soup.select(\n",
    "        \"div.element-border--bottom:nth-child(1) > p:nth-child(n+2)\"\n",
    "    )\n",
    "    subtitles: ResultSet[Tag] = soup.select(\".single__excerpt > p:nth-child(1)\")\n",
    "    headers: ResultSet[Tag] = soup.select(\".typography__display--2\")\n",
    "    tags: list[str] = [\n",
    "        x.get_text() for x in soup.select(\".single__tag-list > ul > li > a\")\n",
    "    ]\n",
    "    author: ResultSet[Tag] | list[str] | str = soup.select(\n",
    "        \".single__author-info > span:nth-child(1) > a\"\n",
    "    )\n",
    "    dates: ResultSet[Tag] = soup.select(\".entry-date\")\n",
    "    text: str = \"\"\n",
    "\n",
    "    if author:\n",
    "        author = [x.get_text() for x in author]\n",
    "    else:\n",
    "        author = \"\"\n",
    "\n",
    "    if dates:\n",
    "        date: str = dates[0].get_text()\n",
    "    else:\n",
    "        date = \"\"\n",
    "\n",
    "    if subtitles:\n",
    "        subtitle: str = subtitles[0].get_text()\n",
    "    else:\n",
    "        subtitle = \"\"\n",
    "\n",
    "    if headers:\n",
    "        header: str = headers[0].get_text()\n",
    "    else:\n",
    "        header = \"\"\n",
    "\n",
    "    for p in paragraph:\n",
    "        text += p.get_text()\n",
    "    text = text.replace(\"\\xa0\", \"\")\n",
    "    text = text.replace(\"CONTINUA DEPOIS DA PUBLICIDADE\", \"\")\n",
    "    text = text.replace(\"\\n\\nRelacionados\\n\", \"\")\n",
    "\n",
    "    output_list[index] = {\n",
    "        \"text\": text,\n",
    "        \"title\": header,\n",
    "        \"subtitle\": subtitle,\n",
    "        \"tags\": tags,\n",
    "        \"date\": date,\n",
    "        \"author\": author,\n",
    "    }\n",
    "\n",
    "\n",
    "input_list = links\n",
    "output_list: ListProxy[None] = multiprocessing.Manager().list([None] * len(input_list))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_processes: int = multiprocessing.cpu_count()\n",
    "\n",
    "    with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "        pool.starmap(\n",
    "            worker, [(i, input_list, output_list) for i in range(len(input_list))]\n",
    "        )\n",
    "\n",
    "result_multi: ListProxy[Any] = output_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For auditing purposes we will be exporting the raw data collected to a csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result: list[Any] = [x for x in result_multi]\n",
    "df = pd.DataFrame(result)\n",
    "df.to_csv(\"infomoney-data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are standardizing the data so all dates follow the `datetime` specifications so we can transform then in datetime for better consumption.\n",
    "\n",
    "> Regarding the `formatTo` function, it was created for the specif case of **Infomoney** since all news dates are in pt-BR and for then to be processed by `datetime.datetime.strptime` they need to be in english.\n",
    "> Also, it treats all the found exceptions, **this code may need to be updated if the website changes it's structure**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatTo(x: str):\n",
    "    month: str = \"\"\n",
    "    if len(x) == 16:\n",
    "        month = x[2:5]\n",
    "    else:\n",
    "        month = x[3:6]\n",
    "\n",
    "    if month == \"jan\":\n",
    "        x = x.replace(\"jan\", \"Jan\")\n",
    "\n",
    "    elif month == \"fev\":\n",
    "        x = x.replace(\"fev\", \"Feb\")\n",
    "\n",
    "    elif month == \"mar\":\n",
    "        x = x.replace(\"mar\", \"Mar\")\n",
    "\n",
    "    elif month == \"abr\":\n",
    "        x = x.replace(\"abr\", \"Apr\")\n",
    "\n",
    "    elif month == \"mai\":\n",
    "        x = x.replace(\"mai\", \"May\")\n",
    "\n",
    "    elif month == \"jun\":\n",
    "        x = x.replace(\"jun\", \"Jun\")\n",
    "\n",
    "    elif month == \"jul\":\n",
    "        x = x.replace(\"jul\", \"Jul\")\n",
    "\n",
    "    elif month == \"ago\":\n",
    "        x = x.replace(\"ago\", \"Aug\")\n",
    "\n",
    "    elif month == \"set\":\n",
    "        x = x.replace(\"set\", \"Sep\")\n",
    "\n",
    "    elif month == \"out\":\n",
    "        x = x.replace(\"out\", \"Oct\")\n",
    "\n",
    "    elif month == \"nov\":\n",
    "        x = x.replace(\"nov\", \"Nov\")\n",
    "\n",
    "    elif month == \"dez\":\n",
    "        x = x.replace(\"dez\", \"Dec\")\n",
    "\n",
    "    if \"Mayo\" in x:\n",
    "        x = x.replace(\"Mayo\", \"May\")\n",
    "\n",
    "    if \"maio\" in x:\n",
    "        x = x.replace(\"maio\", \"May\")\n",
    "\n",
    "    if x == \"\":\n",
    "        return nan\n",
    "\n",
    "    return datetime.strptime(x, \"%d %b %Y %Hh%M\")\n",
    "\n",
    "\n",
    "df[\"date\"] = df[\"date\"].apply(formatTo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning all rows that contain no text, only whitespaces or no tags since they are missing important information.\n",
    "\n",
    "> It could be argued that news without tags are still useful. However, since we do not have enough resources to process so many news items at once, it is better if we ignore those cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask: Series[bool] = (\n",
    "    (df[\"text\"] == \"\") | df[\"text\"].str.match(\"\\s+\") | (df[\"tags\"] == \"[]\")\n",
    ")\n",
    "df: DataFrame = df.drop(df[mask].index).reset_index(drop=True)\n",
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing all instances that could have passed the scraping process which are not in the wanted time frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask: Series[Any] = df[\"date\"].apply(\n",
    "    lambda x: x.year < 2022 or x.year == 2022 and x.month < 9\n",
    ")\n",
    "df: DataFrame = df.drop(df[mask].index).reset_index(drop=True)\n",
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the author column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"author\"] = df[\"author\"].apply(\n",
    "    lambda x: [\n",
    "        author.replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\",\", \"\") for author in x\n",
    "    ]\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting to a csv.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
